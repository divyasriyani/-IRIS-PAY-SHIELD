{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11344332,"sourceType":"datasetVersion","datasetId":7097934}],"dockerImageVersionId":30919,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1234.530165,"end_time":"2025-03-13T07:07:52.681771","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-13T06:47:18.151606","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/sondosaabed/iris-eye-recognition-endtoend-93?scriptVersionId=185982073\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{"papermill":{"duration":0.015045,"end_time":"2025-03-13T06:47:21.015102","exception":false,"start_time":"2025-03-13T06:47:21.000057","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 🔴 **Cite This Notebook** 🔴\n\nIf you find this notebook useful in your research or projects, please consider citing it. Proper citation helps me gain recognition for my work and allows others to follow and build upon it.\n\n**Sondos, _An End-to-end segmentation-free approach Iris Biometric Authentication_, Open Source (GitHub & Kaggle), May 2024. Available at: [https://github.com/sondosaabed/Iris-of-eyes-recognition](https://github.com/sondosaabed/Iris-of-eyes-recognition) and [https://www.kaggle.com/code/sondosaabed/iris-eye-recognition-endtoend-93](https://www.kaggle.com/code/sondosaabed/iris-eye-recognition-endtoend-93)**\n<hr>","metadata":{"papermill":{"duration":0.012887,"end_time":"2025-03-13T06:47:21.041724","exception":false,"start_time":"2025-03-13T06:47:21.028837","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## <div align=\"center\"> Iris eye Recognition</div>\n<div align=\"center\">\nAn (End-to-end segmentation-free approach) Iris Biometric Authentication \n</div>\n<hr>","metadata":{"papermill":{"duration":0.0129,"end_time":"2025-03-13T06:47:21.068273","exception":false,"start_time":"2025-03-13T06:47:21.055373","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Abstract**\n\nIn this project, a Biometric Authentication system using the Iris biometric authentication method is designed. The approach taken is using the CASIA-Thousand-IRIS dataset and model it using the Deep Convultional Neural Network Architicture, with the minimum image-preprocessing such as resizing with keeping the aspect ratiio and normalization. It is an end-to-end technique without performing segmentaion of the IRIS itself. The results are promising, even without perfroing training on augmentation, the testing accuracy has reached **(93.15%)**. Finally, for the proof of the (biometric authentication system concept) a simple mobile application is designed and the model is deployed on it (IrisRecognizer) as it was exported to it's liter version were default quantization is performed. \n<hr>","metadata":{"papermill":{"duration":0.013028,"end_time":"2025-03-13T06:47:21.094450","exception":false,"start_time":"2025-03-13T06:47:21.081422","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Table of Contents\n- Introduction\n    - Aim and Objectives\n    - Methodology\n- Theory\n    - About Dataset Used\n    - Deep Learning for Image Recognition\n- Software Listing\n- Implementation\n    - Dataset Analysis\n        - Loading dataset\n        - Exploring dataset (vidualization, distributions)\n        - Preparing dataset (images, labels, spliting)\n        - experimnt with Augemntaion\n    - Data Modeling (The verifier)\n        - Model Architicture\n        - Traing and Testing\n        - Model Performance (Loss and accuracy)\n        - Testing and saving weights\n        - Evaluation Metrics\n        - Model Exporting\n    - GUI (The authenticator)\n        - First time users, and prefrences\n        - Image accqusition\n        - Image preparing for recognition\n        - Model Inference\n- Discussion and optimization\n- Conclusion\n- Refrences\n\n## List of Figures\n- Figure 1: IRis dataset collection device IKEMB-100 camera \n- Figure 2: Figure: Data Sample from IRIS CaSIA\n- Figure 3: Ramndom Small sample of the dataset\n- Figure 4: Image 50 person label ..\n- Figure 5: Distribution of Image Sizes\n- Figure 6: Distribution of Aspect Ratios\n- Figure 7: Labels Frequency Treemap\n- Figure 8: Preprocessed image sample\n- Figure 9: Augmented image sample\n- Figure 10: Model accurac\n- Figure 11: Loss Learning Curve\n- Figure 12: Prefrences\n- Figure 13: Image acusition\n- Figure 14: Model Answer\n\n## List of Tables\n- Table 1: Software Listenings\n- Table 2: Dataset Head\n- Table 3: Dataset Tail\n- Table 4: Dataset Numerical Describtion\n- Table 5: Missing Values By Percentage\n- Table 6: Dataset Columns Data types\n- Table 7: Number of uniques in the datasets\n- Table 8: Labels Distribution\n\n<hr>","metadata":{"papermill":{"duration":0.013082,"end_time":"2025-03-13T06:47:21.120693","exception":false,"start_time":"2025-03-13T06:47:21.107611","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Introduction**\n\nThe science of Cybersecurity has become essential and irreparable in modern life. With the rise of information technology, the fragility and vulnerability also increases. One of the aspects that cybersecurity addresses is the sophisticated kinds of cybercrime and cyberespionage activities, as well as cyber-terror and cyberwar. Another aspect that cybersecurity addresses is Controlling Access for computer resources, with a known framework called: the triple A’s (AAA) [1] it stands for Authentication, Authorization, and Accounting. This report is concerned with the first A: the Authentication part. Simply put, Authentication is when the user provides information to the system that affirm they are who they claim to be. There are three main types of authentication:\n- Something you know, like a password.\n- Something you have, like a Universal Serial Bus (USB) key.\n- Something you are, such as fingerprint or other biometrics.\n\n![image](https://github.com/sondosaabed/Iris-of-eyes-recognition/assets/65151701/1d752641-d441-4c6d-8c4d-52f0dfa92c38)\n\n**Fig. 1:** Access control: Perceived level of applied security\n\n\nSomething you are or Biometric-based authentication has gained attention in authentication due to what it brought to the table as against somethings you know and somethings you have. It overcame and introduced negative and positive recognition. However, biometric-based raises privacy concerns in terms of collection. That introduces to the organisations who use the biometric-based data protection tasks. In this report, the focus is on Iris biometric authentication. The Iris-Recognition has been widely used in identification for these reasons:\n\n1. `Unique:` there are not any iris having the same physical characteristic as others, even if they come from the same person or identical twins; \n2. `Stability:` the iris is formed during childhood, and it generally maintains unchangeable physical characteristics throughout life; \n3. `Informative:` the iris has rich texture information such as spots, stripes, \f\n\nlaments and coronas.\n4. `Safety:` Since the iris is located in a circular area under the surface of the eye between the black pupil and the white sclera, it is rarely disturbed by external factors. As a result, it is di\u000ecult to forge the iris pattern; \n5. `Contactless:` Iris Recognition (IR) is more hygienic than biometrics that requires contact, such as fngerprint recognition.","metadata":{"papermill":{"duration":0.013085,"end_time":"2025-03-13T06:47:21.147355","exception":false,"start_time":"2025-03-13T06:47:21.134270","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Background\nThe anatomy of the human eye has inspired computer vision tasks from the beginning. As shown in the following figure, the Iris is located between the white sclera and the cornea. That is a contactless area from the human face. It is worth mentioning that left and right iris have distinctive patterns too for the same person.\n\n![image](https://github.com/sondosaabed/Iris-of-eyes-recognition/assets/65151701/27f38752-b6cc-42ea-84d6-707e4804f379)\n\n**Fig. 2:** Human-Eye Anatomy","metadata":{"papermill":{"duration":0.013039,"end_time":"2025-03-13T06:47:21.173585","exception":false,"start_time":"2025-03-13T06:47:21.160546","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Iris Recognition Technology \nIris Recognition technology is the use of automated methods of biometric identification that depends on mathematical induction of rules that define such complex patterns. The idea is to minimise the the intra which is the patterns (within) that helps identify the person, while also maximise the inter which is the patterns (between) that differs that person from the other one. The process starts by acquisition stage, it could be done using different devices such as the one shown in the following which is a secure Kit used portable for enrollment and recognition:\n\n![image](https://github.com/sondosaabed/Iris-of-eyes-recognition/assets/65151701/f63fc668-05d1-4a6d-9d2c-34fd91fdf19b)\n**Fig. 3:** Iris scanner PIER 2.3 (Portable Iris Enrollment and Recognition) from SecuriMetrics\n\nOnce the individual Iris is acquired using these devices, the features then are extracted. It could be implicitly or explicitly extracted to create a unique template matrix. The template will then be matched and compared against the other templates. The Authorization is granted or denied based on the Authentication of the Iris results. \n\nTwo Iris templates from the same eye will be forming a genuine pair. While two different eyes will form an imposter pair. The matching comparison between the templates includes correlation ana;ysis, both the genuine pairs and the imposter are correlated, but the correlation from the same eye are stronger.\n","metadata":{"papermill":{"duration":0.013578,"end_time":"2025-03-13T06:47:21.200644","exception":false,"start_time":"2025-03-13T06:47:21.187066","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Problem Statemnt\n\nAbout the Problem Statemnet of this project: This is a classification problem where we have mutli persons each have left eye or right eyes Iris. Mainly teh dataset contains 1000 persons with Iris images of the Left and the Right eye. In total there is 2000 unique labels.\n\n## Approach\nAs for the approach in this implenetauion, is an  End-to-end segmentaion free approach using deep learning for identifiation of Iris. Where a base model is used as the Feature extractor and the Dense softmax are used for the classification task.","metadata":{"papermill":{"duration":0.013829,"end_time":"2025-03-13T06:47:21.227990","exception":false,"start_time":"2025-03-13T06:47:21.214161","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Aim and Objectives\n\n- To design a biometric based authentication system.\n- To perform data analysis on Iris Dataset\n- To perfrom data modeling as a user recognition task (Verifier Module) \n- To build an enrollment module (Enrollment Module)\n\n<hr>","metadata":{"papermill":{"duration":0.013525,"end_time":"2025-03-13T06:47:21.256349","exception":false,"start_time":"2025-03-13T06:47:21.242824","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Theory**\n\n### About Dataset used\n\nCASIA-Iris-Thousand contains 20,000 iris images from 1,000 subjects, which were collected using IKEMB-100 camera (Figure ) produced by IrisKing. IKEMB-100 is a dual-eye iris camera with friendly visual feedback, realizing the effect of “What You See Is What You Get”. The bounding boxes shown in the frontal LCD help users adjust their pose for high-quality iris image acquisition. The main sources of intra-class variations in CASIA-Iris-Thousand are eyeglasses and specular reflections. Since CASIA-Iris-Thousand is the first publicly available iris dataset with one thousand subjects, it is well-suited for studying the uniqueness of iris features and develop novel iris classification and indexing methods.\n\n<div align=\"center\">\n    <p>Figure: IRis dataset collection device IKEMB-100 camera [3]</p>\n    <img src=\"https://hycasia.github.io/dataset/casia-irisv4/V4Fig.8.jpg\" alt=\"Iris collection device\"/>\n    <p>Figure: Data Sample from IRIS CaSIA [3]</p>\n    <img src=\"https://hycasia.github.io/dataset/casia-irisv4/V4Fig.9.jpg\" alt=\"Data sample\"/>\n</div>\n\n### Deep Learning for Image recognition\n\nDeep learning is widly used in such feilds of authentication. CNN specifically has revolutionalized the feilds of image recognition. An Iris recognition framework based on CNN features is widly used alongside image preprocessing.\n\nThere are diffrent architicutres used in litrature research, however this architicure uses the minimum preprocessing techniques. \n\nThere are also diffrent dataset that are there, the reason that CASIA was used is the the data is balanced\n<hr>","metadata":{"papermill":{"duration":0.013439,"end_time":"2025-03-13T06:47:21.283839","exception":false,"start_time":"2025-03-13T06:47:21.270400","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Software Listing**\n\nIn this project many tools were used and utilized. The followning table shows the software tools and versions used in this project.\n\nTable 1: Software Listenings\n\n| Tools   | Version   |\n|-------|-----------|\n|Python|3.12.2|\n| pandas | 2.2.2 |\n| numpy    | 1.26.4 |\n| Pillow (PIL) | 9.5.0 |\n| TensorFlow | 2.15.0 |\n| OpenCV (cv2) | 4.9.0 |\n| Keras (TensorFlow) | updated |\n| matplotlib   | updated |\n| scikit-learn | updated |\n| squarify | updated |\n|Visual Studio Code| Updated|\n|Git & github| https://github.com/sondosaabed/Iris-of-eyes-recognition|\n|Android Studio|https://github.com/sondosaabed/IrisRecognizer|\n|torch|Updated|\n|torchmetric|Updated|\n<hr>","metadata":{"papermill":{"duration":0.013414,"end_time":"2025-03-13T06:47:21.311393","exception":false,"start_time":"2025-03-13T06:47:21.297979","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Implemetation**\n\nIn this section, the aim and objectives are met and presented. First the dataset is looked into and decisions are made based on that, then the model archeiticure is prepared and finally the enrollment module is used and the model is integrated into it.","metadata":{"papermill":{"duration":0.012663,"end_time":"2025-03-13T06:47:21.337749","exception":false,"start_time":"2025-03-13T06:47:21.325086","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Necceary imports","metadata":{"papermill":{"duration":0.012718,"end_time":"2025-03-13T06:47:21.364093","exception":false,"start_time":"2025-03-13T06:47:21.351375","status":"completed"},"tags":[]}},{"cell_type":"code","source":"## Visualization, sampling and data loading\nimport squarify \nimport os\nimport PIL\nimport math\nimport cv2\nimport random\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n## Data modeling\nimport tensorflow as tf\nfrom keras import Sequential\nfrom sklearn.model_selection import cross_val_score\nfrom keras import backend as K\nfrom tensorflow import keras as keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score,  roc_curve, confusion_matrix\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Layer, Input, Dense,Dropout, Conv2D, BatchNormalization, Flatten, Input, Conv2D, GaussianNoise,MaxPooling2D, Flatten, Dense, Dropout\n\n### Model evaluatoin\nfrom torchmetrics import Precision, Recall, Accuracy, AUROC\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='sklearn.metrics')\n\nimport numpy as np\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:00:44.175909Z","iopub.execute_input":"2025-04-15T15:00:44.176574Z","iopub.status.idle":"2025-04-15T15:00:44.183032Z","shell.execute_reply.started":"2025-04-15T15:00:44.176538Z","shell.execute_reply":"2025-04-15T15:00:44.182403Z"},"papermill":{"duration":25.640761,"end_time":"2025-03-13T06:47:47.018496","exception":false,"start_time":"2025-03-13T06:47:21.377735","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Dataset Analysis**\nThis is the first part of the impelemntaion, it contains the whole data analysis process. Starting with reading the data, exploring the data, preprocessing the data (including images and lables) then finally preparing it for traing (including spliting into train and test). Will also experiment with data augmentaion.","metadata":{"papermill":{"duration":0.013168,"end_time":"2025-03-13T06:47:47.045953","exception":false,"start_time":"2025-03-13T06:47:47.032785","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Loading Dataset\nIn this section the dataset files and laels are loaded into a pandas dataframe.","metadata":{"papermill":{"duration":0.012842,"end_time":"2025-03-13T06:47:47.072213","exception":false,"start_time":"2025-03-13T06:47:47.059371","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_dataset(path):\n    \"\"\"\n    Loading the dataset into a pandas datframe.\n    The images of CASIA-Iris-Thousand are stored as:    /CASIA-Iris-Thousand/YYY/E/S5YYYENN.jpg\n\n        YYY: the unique identifier of the subject in the subset\n        E: ‘L’ denotes left eye and ‘R’ denotes right eye\n        NN: the index of the image in the class\n    \n    Args:\n        path(str): string that has the path of the dataset\n    Returns:\n        df(pd.DataFrame): the loaded dataframe\n    \"\"\"\n    labels = []\n    images = []\n\n    for folder in os.listdir(path):\n        for lr in os.listdir(path+'/'+folder): #left or right\n            for image in os.listdir(path+'/'+folder+'/'+lr):\n                if image.endswith('b') is False:\n                    images.append(path+'/'+folder+'/'+lr+'/'+image)\n                    labels.append(folder+'-'+lr) #+'-'+lr\n\n    df = pd.DataFrame(list(zip(labels, images)), columns=['Label', 'ImagePath'])\n    return df, labels, images","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:00:52.726525Z","iopub.execute_input":"2025-04-15T15:00:52.726817Z","iopub.status.idle":"2025-04-15T15:00:52.732188Z","shell.execute_reply.started":"2025-04-15T15:00:52.726787Z","shell.execute_reply":"2025-04-15T15:00:52.731247Z"},"papermill":{"duration":0.060861,"end_time":"2025-03-13T06:47:47.146561","exception":false,"start_time":"2025-03-13T06:47:47.085700","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df, labels, images = load_dataset('/kaggle/input/thosuand-iris/CASIA-Iris-Thousand/CASIA-Iris-Thousand')","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:00.424653Z","iopub.execute_input":"2025-04-15T15:01:00.424934Z","iopub.status.idle":"2025-04-15T15:01:10.617655Z","shell.execute_reply.started":"2025-04-15T15:01:00.424912Z","shell.execute_reply":"2025-04-15T15:01:10.616723Z"},"papermill":{"duration":13.074677,"end_time":"2025-03-13T06:48:00.235018","exception":false,"start_time":"2025-03-13T06:47:47.160341","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Expolring Dataset\nThis section includes looking into the visulaization, distributions, checking for missing and duplicates values. ","metadata":{"papermill":{"duration":0.013997,"end_time":"2025-03-13T06:48:00.263319","exception":false,"start_time":"2025-03-13T06:48:00.249322","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def missing_values(df):\n    \"\"\"\n    This is to get the percetages of missing data\n    Args:\n        df (pd.Dataframe): contains the data\n    Returns:\n        missing_percetanges(pd.Dataframe): contains Column,\tCounts, and\tPercentage\n            of the missing values for eah colmn\n    \"\"\"\n    missing_count = df.isnull().sum()\n    missing_percetanges = pd.DataFrame({\n        'Column': missing_count.index,\n        'Counts': missing_count.values,\n        'Percentage': (missing_count.values / len(df)) * 100  \n    })\n    return  missing_percetanges","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:14.555492Z","iopub.execute_input":"2025-04-15T15:01:14.555778Z","iopub.status.idle":"2025-04-15T15:01:14.560079Z","shell.execute_reply.started":"2025-04-15T15:01:14.555757Z","shell.execute_reply":"2025-04-15T15:01:14.559109Z"},"papermill":{"duration":0.020335,"end_time":"2025-03-13T06:48:00.297784","exception":false,"start_time":"2025-03-13T06:48:00.277449","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def explore_data(df):\n    \"\"\"\n    Exploring a dataset sample\n    Args:\n        sample (pd.Dataframe): the dataset sample to explore.\n    Returns:\n        results (dict): containing results of each exploration with the title as key\n    \"\"\"\n    head = pd.DataFrame(df.head())\n    tail = pd.DataFrame(df.tail())\n    nunique = pd.DataFrame(df.nunique(), columns=[\"#_of_Unique\"])\n    describe = pd.DataFrame(df.describe())\n    dtypes =  pd.DataFrame(df.dtypes, columns=[\"Datatype\"])\n    labels_distribution = pd.DataFrame(df['Label'].value_counts())\n    results = {\n        'Table 2: Dataset Head:':head,\n        'Table 3: Dataset Tail:':tail,\n        'Table 4: Dataset Numerical Describtion: ':describe,\n        'Table 5: Missing Values By Percentage': missing_values(df), \n        'Table 6: Dataset Columns Data types: ':dtypes,\n        'Table 7: Number of uniques in the datasets:':nunique,\n        'Table 8: Labels Distribution:':labels_distribution}\n    return results","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:24.009703Z","iopub.execute_input":"2025-04-15T15:01:24.009988Z","iopub.status.idle":"2025-04-15T15:01:24.014802Z","shell.execute_reply.started":"2025-04-15T15:01:24.009969Z","shell.execute_reply":"2025-04-15T15:01:24.013963Z"},"papermill":{"duration":0.01973,"end_time":"2025-03-13T06:48:00.331621","exception":false,"start_time":"2025-03-13T06:48:00.311891","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_dataset_exploration(results):\n    \"\"\"\n    Prints a beautufil display of each of the exploration dataframe\n    Args:\n        results (dict): contains exploration outputs with the title as key\n    Returns:\n        nothing\n    \"\"\"\n    for operation, dataframe in results.items():\n        print(f\"{operation}\")\n        if operation == 'Table 6: Missing Values By Percentage':\n            print(\"Total Sum of Missing Percetange: \", dataframe['Percentage'].sum())\n        display(dataframe)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:29.679048Z","iopub.execute_input":"2025-04-15T15:01:29.679327Z","iopub.status.idle":"2025-04-15T15:01:29.683628Z","shell.execute_reply.started":"2025-04-15T15:01:29.679307Z","shell.execute_reply":"2025-04-15T15:01:29.682751Z"},"papermill":{"duration":0.019401,"end_time":"2025-03-13T06:48:00.365980","exception":false,"start_time":"2025-03-13T06:48:00.346579","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print_dataset_exploration(explore_data(df))","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:33.771018Z","iopub.execute_input":"2025-04-15T15:01:33.771293Z","iopub.status.idle":"2025-04-15T15:01:33.853781Z","shell.execute_reply.started":"2025-04-15T15:01:33.771272Z","shell.execute_reply":"2025-04-15T15:01:33.853153Z"},"papermill":{"duration":0.108453,"end_time":"2025-03-13T06:48:00.488367","exception":false,"start_time":"2025-03-13T06:48:00.379914","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> In this initial data exploration, it is found out that the dataset has two cloumns. The first one contains the label of the Iris image: that label is in the form of (Subject XXX-Y) and the Y could be either left eye or right eye of the same subject. \n\n> The datset was checked for missing values and duplicates. There was no such found. We have 2000 unique labels and 20000 images. The fisrt look into the distribution seems fair, since there is 10 images for that same label.","metadata":{"papermill":{"duration":0.015095,"end_time":"2025-03-13T06:48:00.521483","exception":false,"start_time":"2025-03-13T06:48:00.506388","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"##### Data Visualization \n\nnow let's take a deeper look into the datset:\n\n- Images Visualizations","metadata":{"papermill":{"duration":0.014201,"end_time":"2025-03-13T06:48:00.550679","exception":false,"start_time":"2025-03-13T06:48:00.536478","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def show_random_samples(df, num):\n    \"\"\"\n    Shows a sample on the dataframe in a specific location.\n    Args:\n        df (pd.DataFrame): the dataset\n        num (int): the number of random samples to display\n    Return:\n        Nothing but shows a sample in the display\n    \"\"\"\n    random.seed(1190652)\n    random_indices = random.sample(range(df.shape[0]), num)\n    num_rows = math.ceil(num / 4) \n\n    fig, axes = plt.subplots(num_rows, 4, figsize=(20, num_rows * 5))  \n    for i, idx in enumerate(random_indices):\n        row = i // 4\n        col = i % 4\n        \n        if idx < df.shape[0]:  \n            image_path =  df.loc[idx, \"ImagePath\"]\n            image = PIL.Image.open(image_path)\n            ax = axes[row, col] if num_rows > 1 else axes[col]\n            ax.imshow(image, cmap='gray')\n            ax.set_title(f\"Image {idx} Person Label: {df.loc[idx, 'Label']}\")\n            ax.axis(\"off\")\n\n    plt.suptitle(\"Figure 3: Random Small Sample of the Dataset\")\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:39.521325Z","iopub.execute_input":"2025-04-15T15:01:39.521663Z","iopub.status.idle":"2025-04-15T15:01:39.527661Z","shell.execute_reply.started":"2025-04-15T15:01:39.521640Z","shell.execute_reply":"2025-04-15T15:01:39.526663Z"},"papermill":{"duration":0.022529,"end_time":"2025-03-13T06:48:00.587480","exception":false,"start_time":"2025-03-13T06:48:00.564951","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_random_samples(df, 8)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:43.977069Z","iopub.execute_input":"2025-04-15T15:01:43.977400Z","iopub.status.idle":"2025-04-15T15:01:45.302676Z","shell.execute_reply.started":"2025-04-15T15:01:43.977344Z","shell.execute_reply":"2025-04-15T15:01:45.301553Z"},"papermill":{"duration":1.451691,"end_time":"2025-03-13T06:48:02.054346","exception":false,"start_time":"2025-03-13T06:48:00.602655","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> It is noticed that this random image contains diffrent features of such subjects in which thta:\n- Some eyes are left samples and some eyes are right samples.\n- Some eyes have glasses.\n- Some eyes uses eyeliner.\n- Some eyes interval are big.\n- Some eyes intervals are small.\n- The images are greyscaled.","metadata":{"papermill":{"duration":0.041683,"end_time":"2025-03-13T06:48:02.138708","exception":false,"start_time":"2025-03-13T06:48:02.097025","status":"completed"},"tags":[]}},{"cell_type":"code","source":"image_path =  df.loc[50, \"ImagePath\"]\nimage = PIL.Image.open(image_path)\nplt.imshow(image, cmap='gray')\nplt.title(f\"Figure 4: Image {50} Person Label: {df.loc[50, 'Label']}\")\nplt.axis(\"off\")\nwidth, height = image.size\nprint(\"The width of the images: \", width)\nprint(\"The height of the images: \", height)\nprint(\"Shape\", image.size)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:50.555346Z","iopub.execute_input":"2025-04-15T15:01:50.555655Z","iopub.status.idle":"2025-04-15T15:01:50.733571Z","shell.execute_reply.started":"2025-04-15T15:01:50.555633Z","shell.execute_reply":"2025-04-15T15:01:50.732787Z"},"papermill":{"duration":0.247334,"end_time":"2025-03-13T06:48:02.427769","exception":false,"start_time":"2025-03-13T06:48:02.180435","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Data Distrubutions\nLet's take a look at the images sizes, aspect ratios and the labels distributions","metadata":{"papermill":{"duration":0.046051,"end_time":"2025-03-13T06:48:02.520360","exception":false,"start_time":"2025-03-13T06:48:02.474309","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"- Calcutae sizes and aspect ratios:","metadata":{"papermill":{"duration":0.045226,"end_time":"2025-03-13T06:48:02.611709","exception":false,"start_time":"2025-03-13T06:48:02.566483","status":"completed"},"tags":[]}},{"cell_type":"code","source":"image_sizes = []\naspect_ratios = []\n\nfor image_path in df['ImagePath']:\n    image = PIL.Image.open(image_path)\n    width, height = image.size\n    image_sizes.append(width * height)\n    aspect_ratios.append(width / height)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:01:55.328741Z","iopub.execute_input":"2025-04-15T15:01:55.329027Z","iopub.status.idle":"2025-04-15T15:02:57.333829Z","shell.execute_reply.started":"2025-04-15T15:01:55.329005Z","shell.execute_reply":"2025-04-15T15:02:57.333155Z"},"papermill":{"duration":72.975506,"end_time":"2025-03-13T06:49:15.631099","exception":false,"start_time":"2025-03-13T06:48:02.655593","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Distribution of images sizes: The images sizes all are in standarized to the same size as the following figure shows that they are unformed:","metadata":{"papermill":{"duration":0.042353,"end_time":"2025-03-13T06:49:15.717714","exception":false,"start_time":"2025-03-13T06:49:15.675361","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.hist(image_sizes, label=\"Image Size\")\nplt.title(\"Figure 5: Distribution of Image Sizes\")\nplt.xlabel(\"Image Size\")\nplt.ylabel(\"Frequency\")","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:23.693668Z","iopub.execute_input":"2025-04-15T15:03:23.693972Z","iopub.status.idle":"2025-04-15T15:03:23.940197Z","shell.execute_reply.started":"2025-04-15T15:03:23.693951Z","shell.execute_reply":"2025-04-15T15:03:23.939462Z"},"papermill":{"duration":0.323587,"end_time":"2025-03-13T06:49:16.082797","exception":false,"start_time":"2025-03-13T06:49:15.759210","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Distribution of aspect ratios: let's just check teh aspect ratio if it's also the same. The follwing figure shows that they all have a unform aspect ratio which is 1.3:","metadata":{"papermill":{"duration":0.044398,"end_time":"2025-03-13T06:49:16.172103","exception":false,"start_time":"2025-03-13T06:49:16.127705","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.hist(aspect_ratios, label=\"Aspect Ratio\")\nplt.title(\"Figure 6: Distribution of Aspect Ratios\")\nplt.xlabel(\"Aspect Ratio\")\nplt.ylabel(\"Frequency\")\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:28.601488Z","iopub.execute_input":"2025-04-15T15:03:28.601778Z","iopub.status.idle":"2025-04-15T15:03:28.900033Z","shell.execute_reply.started":"2025-04-15T15:03:28.601756Z","shell.execute_reply":"2025-04-15T15:03:28.898906Z"},"papermill":{"duration":0.340269,"end_time":"2025-03-13T06:49:16.556569","exception":false,"start_time":"2025-03-13T06:49:16.216300","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Labels distribution visualizations: the follwing frequency treemap is used to show the labels distribution. We have also unformed distribution, as each label has the same value counts (10).","metadata":{"papermill":{"duration":0.047557,"end_time":"2025-03-13T06:49:16.650652","exception":false,"start_time":"2025-03-13T06:49:16.603095","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure(figsize=(30, 15))\nsquarify.plot(sizes=df['Label'].value_counts(), label=df['Label'].unique())\nplt.axis('off')\nplt.title('Figure 7: Labels Frequency Treemap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:32.605101Z","iopub.execute_input":"2025-04-15T15:03:32.605446Z","iopub.status.idle":"2025-04-15T15:03:38.969493Z","shell.execute_reply.started":"2025-04-15T15:03:32.605418Z","shell.execute_reply":"2025-04-15T15:03:38.968442Z"},"papermill":{"duration":7.311597,"end_time":"2025-03-13T06:49:24.009800","exception":false,"start_time":"2025-03-13T06:49:16.698203","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> The dataset has balanced distributions, no missing or duplicated values are found. There will be minimum data prepartion decisions.","metadata":{"papermill":{"duration":0.081019,"end_time":"2025-03-13T06:49:24.175795","exception":false,"start_time":"2025-03-13T06:49:24.094776","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Preparing Dataset\nInludes sliting into train, test and validation, preprocessing the images to consitsnat size, encoding the labels.","metadata":{"papermill":{"duration":0.083937,"end_time":"2025-03-13T06:49:24.342935","exception":false,"start_time":"2025-03-13T06:49:24.258998","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"##### Images preparing\n- In the datset exploration, no missing images, and all the images have the same sizes and the same aspect ratios. However, due to processing resources limitations the decision is to resize all the images to a consistent size but smaller than that is now. The resizing algortihm is distortion free the sizes are changed but the aspect ratio is the same using pading values.","metadata":{"papermill":{"duration":0.08114,"end_time":"2025-03-13T06:49:24.508557","exception":false,"start_time":"2025-03-13T06:49:24.427417","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Dataset Config\nSIZE = 20000\nNUM_CLASSES = 2000\nIMG_HEIGHT = 150\nIMG_WIDTH = 150\nNUM_CHANNELS = 1\ninput_shape=(IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:46.141742Z","iopub.execute_input":"2025-04-15T15:03:46.142073Z","iopub.status.idle":"2025-04-15T15:03:46.146110Z","shell.execute_reply.started":"2025-04-15T15:03:46.142043Z","shell.execute_reply":"2025-04-15T15:03:46.145119Z"},"papermill":{"duration":0.088516,"end_time":"2025-03-13T06:49:24.678141","exception":false,"start_time":"2025-03-13T06:49:24.589625","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Resizing is done with keeping the aspect ratio","metadata":{"papermill":{"duration":0.086431,"end_time":"2025-03-13T06:49:24.849045","exception":false,"start_time":"2025-03-13T06:49:24.762614","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def resize_keep_aspect_ration(img, target_height=IMG_HEIGHT, target_width=IMG_WIDTH, pad_value=255):\n    \"\"\"\n    Resize an image to a specific size keeping the aspect ratio using padding\n    Args:\n        - img (np.array): image\n        - target_height (int): with the deafult value as IMG_HEIGHT\n        - target_width (int): with the deafult value as IMG_WIDTH\n        - pad_value (int): the padding value foe the left of the image with the deafult value is 255\n    Returns:\n        - The resized image\n    \"\"\"\n    aspect_ratio = img.shape[1] / img.shape[0]\n    \n    ## Get the new sizes with keeping the aspect ratio\n    if aspect_ratio > target_width / target_height:\n        new_width = target_width\n        new_height = int(target_width / aspect_ratio)\n    else:\n        new_height = target_height\n        new_width = int(target_height * aspect_ratio)\n\n    resized_img = cv2.resize(img, (new_width, new_height))\n    \n    ## Padding up and down so the image is in the middle\n    preprocessed_img = np.full((target_height, target_width), pad_value, dtype=np.uint8)\n    x_offset = (target_width - new_width) // 2\n    y_offset = (target_height - new_height) // 2\n    preprocessed_img[y_offset:y_offset + new_height, x_offset:x_offset + new_width] = resized_img\n    \n    return preprocessed_img","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:50.122126Z","iopub.execute_input":"2025-04-15T15:03:50.122488Z","iopub.status.idle":"2025-04-15T15:03:50.128011Z","shell.execute_reply.started":"2025-04-15T15:03:50.122459Z","shell.execute_reply":"2025-04-15T15:03:50.126887Z"},"papermill":{"duration":0.089272,"end_time":"2025-03-13T06:49:25.025286","exception":false,"start_time":"2025-03-13T06:49:24.936014","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- The image preprocessing function will include:\n    - Read with greyscale.\n    - Resizing with keeping the aspcet ratio.\n    - Normalization. [0, 1]","metadata":{"papermill":{"duration":0.085157,"end_time":"2025-03-13T06:49:25.193836","exception":false,"start_time":"2025-03-13T06:49:25.108679","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_image(img_dir):\n    \"\"\"\n    Resizing imgaes with keeping the aspect ratio.\n    Args:\n        img_dir(str): Image path on the dataset\n        target_height(int): the targeted height to be resized\n        target_width(int): the targeted width to be resized\n        pad_value(int): used as the padding value for the resized image\n    Returns:\n        preprocessed_img (cv.Imgae): processsed image\n    \"\"\"\n    img = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    img = resize_keep_aspect_ration(img)\n    img = img/255.\n    \n    return img","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:54.029792Z","iopub.execute_input":"2025-04-15T15:03:54.030086Z","iopub.status.idle":"2025-04-15T15:03:54.034270Z","shell.execute_reply.started":"2025-04-15T15:03:54.030065Z","shell.execute_reply":"2025-04-15T15:03:54.033464Z"},"papermill":{"duration":0.09177,"end_time":"2025-03-13T06:49:25.370438","exception":false,"start_time":"2025-03-13T06:49:25.278668","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- This is an example of performing image resizing output:","metadata":{"papermill":{"duration":0.082247,"end_time":"2025-03-13T06:49:25.539819","exception":false,"start_time":"2025-03-13T06:49:25.457572","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.imshow(preprocess_image(df.iloc[15]['ImagePath']), cmap=\"gray\")\nplt.title(\"Figure 8: Preprocessed image sample\")\nplt.show","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:03:58.745308Z","iopub.execute_input":"2025-04-15T15:03:58.745670Z","iopub.status.idle":"2025-04-15T15:03:59.041683Z","shell.execute_reply.started":"2025-04-15T15:03:58.745644Z","shell.execute_reply":"2025-04-15T15:03:59.040864Z"},"papermill":{"duration":0.386652,"end_time":"2025-03-13T06:49:26.011102","exception":false,"start_time":"2025-03-13T06:49:25.624450","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### labels preparing:\n- For the labels preparing for training, not much is needed. There was no missing values, However the datatype of the labels will be strings instead of numerical. The label will be considered as a string (person name) and that will be encoded. The distribution of the labels was also looked into and it was found that the labesl are equally distribuation and no customized data augmentation is needed. ","metadata":{"papermill":{"duration":0.087402,"end_time":"2025-03-13T06:49:26.186919","exception":false,"start_time":"2025-03-13T06:49:26.099517","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def preprocess_labels(df):\n    \"\"\"\n    Prepares labels for training indluding transform to string and then encode.\n    Args: \n        df(pd.DataFrame): dataset non-encoded labels and images paths\n    Rteurns:\n        labels(np.array): prepared labels for training\n    \"\"\"\n    labels = df['Label'].astype(str)\n    le = LabelEncoder()\n    le.fit(labels)\n    labels = le.transform(labels)\n    return labels","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:04:03.424407Z","iopub.execute_input":"2025-04-15T15:04:03.424690Z","iopub.status.idle":"2025-04-15T15:04:03.428732Z","shell.execute_reply.started":"2025-04-15T15:04:03.424671Z","shell.execute_reply":"2025-04-15T15:04:03.427953Z"},"papermill":{"duration":0.092546,"end_time":"2025-03-13T06:49:26.364028","exception":false,"start_time":"2025-03-13T06:49:26.271482","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Example of preprocessing the labels into encoded labels:","metadata":{"papermill":{"duration":0.085742,"end_time":"2025-03-13T06:49:26.536270","exception":false,"start_time":"2025-03-13T06:49:26.450528","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"Label after encoding: \", preprocess_labels(df)[120])\nprint(\"Label before encoding: \",df.iloc[120]['Label'])","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:04:07.865810Z","iopub.execute_input":"2025-04-15T15:04:07.866101Z","iopub.status.idle":"2025-04-15T15:04:07.878669Z","shell.execute_reply.started":"2025-04-15T15:04:07.866081Z","shell.execute_reply":"2025-04-15T15:04:07.877774Z"},"papermill":{"duration":0.101276,"end_time":"2025-03-13T06:49:26.723377","exception":false,"start_time":"2025-03-13T06:49:26.622101","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Split the dataset into trainig, testing and validation dataset. The splitting rule is 70:15:15","metadata":{"papermill":{"duration":0.095793,"end_time":"2025-03-13T06:49:26.910053","exception":false,"start_time":"2025-03-13T06:49:26.814260","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def split_dataset(preprocessed_images, preprocessed_labels, train_size=0.8, validation_size=0.1, shuffle=True):\n    \"\"\"\n    Returns the splitted dataset using the 80:10:10 split rule (for deep learnig we do train split) Pareto principle\n    Args:\n        preprocessed_images()\n        preprocessed_labels\n    Returns:\n        x_train(np.Array): training images\n        x_valid(np.Array): validation images\n        x_test(np.Array): testing images\n        y_train(np.Array): training labels\n        y_valid(np.Array): validation labels\n        y_test(np.Array): testing labeles\n    \"\"\"\n    np.random.seed(1190652)\n    indices = np.arange(SIZE)\n    if shuffle:\n        np.random.shuffle(indices)\n    \n    train_samples = int(SIZE * train_size)\n    validation_samples = int(SIZE * validation_size)\n    \n    train_indices = indices[:train_samples]\n    validation_indices = indices[train_samples:train_samples + validation_samples]\n    test_indices = indices[train_samples + validation_samples:]\n    \n    x_train  = preprocessed_images[train_indices]\n    y_train = preprocessed_labels[train_indices]\n    x_valid = preprocessed_images[validation_indices]\n    y_valid = preprocessed_labels[validation_indices]\n    x_test = preprocessed_images[test_indices]\n    y_test = preprocessed_labels[test_indices]\n    \n    return x_train, x_valid, x_test, y_train, y_valid, y_test","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:04:11.755238Z","iopub.execute_input":"2025-04-15T15:04:11.755552Z","iopub.status.idle":"2025-04-15T15:04:11.760783Z","shell.execute_reply.started":"2025-04-15T15:04:11.755527Z","shell.execute_reply":"2025-04-15T15:04:11.760022Z"},"papermill":{"duration":0.092554,"end_time":"2025-03-13T06:49:27.091074","exception":false,"start_time":"2025-03-13T06:49:26.998520","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Dataset preparation for the training","metadata":{"papermill":{"duration":0.085892,"end_time":"2025-03-13T06:49:27.261285","exception":false,"start_time":"2025-03-13T06:49:27.175393","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def prepare_dataset(df):\n    \"\"\"\n    Prepares the dataset to training and modeling\n    Args:\n        df(pd.DataFrame): the dataset\n    Returns:\n        x_train(np.Array): training images\n        x_valid(np.Array): validation images\n        x_test(np.Array): testing images\n        y_train(np.Array): training labels\n        y_valid(np.Array): validation labels\n        y_test(np.Array): testing labeles\n    \"\"\"\n    preprocessed_images = []\n    for i in range(SIZE):\n        image = preprocess_image(images[i])\n        preprocessed_images.append(image)\n    \n    preprocessed_images = np.array(preprocessed_images).reshape(-1, IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n    preprocessed_labels = preprocess_labels(df)\n    return split_dataset(preprocessed_images, preprocessed_labels)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:04:15.653255Z","iopub.execute_input":"2025-04-15T15:04:15.653553Z","iopub.status.idle":"2025-04-15T15:04:15.658173Z","shell.execute_reply.started":"2025-04-15T15:04:15.653530Z","shell.execute_reply":"2025-04-15T15:04:15.657268Z"},"papermill":{"duration":0.096943,"end_time":"2025-03-13T06:49:27.445995","exception":false,"start_time":"2025-03-13T06:49:27.349052","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train, x_valid, x_test, y_train, y_valid, y_test = prepare_dataset(df)\nprint(\"Triaing set size: \", x_train.shape)\nprint(\"Validation set size: \", x_valid.shape)\nprint(\"Testing set size: \",x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:04:19.248308Z","iopub.execute_input":"2025-04-15T15:04:19.248643Z","iopub.status.idle":"2025-04-15T15:05:05.188349Z","shell.execute_reply.started":"2025-04-15T15:04:19.248618Z","shell.execute_reply":"2025-04-15T15:05:05.187682Z"},"papermill":{"duration":53.731828,"end_time":"2025-03-13T06:50:21.266400","exception":false,"start_time":"2025-03-13T06:49:27.534572","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Data Augmentaion\nHere is one of the experimnet conduted, was training on augmneted images. Which for some reason faild to give better results. The thought is it might be due to the technique used itself or due to the task, it could need data synthesizing instead of data augmentaiion.","metadata":{"papermill":{"duration":0.086292,"end_time":"2025-03-13T06:50:21.436751","exception":false,"start_time":"2025-03-13T06:50:21.350459","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CentralCrop(Layer):\n    def __init__(self, central_fraction=0.5):\n        super(CentralCrop, self).__init__()\n        self.central_fraction = central_fraction\n\n    def call(self, inputs):\n        cropped = tf.image.central_crop(inputs, central_fraction=self.central_fraction)\n        return cropped","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:08:54.399687Z","iopub.execute_input":"2025-04-15T15:08:54.400008Z","iopub.status.idle":"2025-04-15T15:08:54.404778Z","shell.execute_reply.started":"2025-04-15T15:08:54.399983Z","shell.execute_reply":"2025-04-15T15:08:54.403730Z"},"papermill":{"duration":0.092449,"end_time":"2025-03-13T06:50:21.613697","exception":false,"start_time":"2025-03-13T06:50:21.521248","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_augmentation = keras.Sequential([\n    Input(shape=input_shape),\n    CentralCrop(central_fraction=0.5)  \n])\n\naugmented_images = data_augmentation(x_train)  ","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:08:57.569156Z","iopub.execute_input":"2025-04-15T15:08:57.569463Z","iopub.status.idle":"2025-04-15T15:09:02.411796Z","shell.execute_reply.started":"2025-04-15T15:08:57.569441Z","shell.execute_reply":"2025-04-15T15:09:02.410872Z"},"papermill":{"duration":5.145855,"end_time":"2025-03-13T06:50:26.848153","exception":false,"start_time":"2025-03-13T06:50:21.702298","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Here is an example of the augmented dataset","metadata":{"papermill":{"duration":0.082466,"end_time":"2025-03-13T06:50:27.015238","exception":false,"start_time":"2025-03-13T06:50:26.932772","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f'Encoded Person Label is: {y_valid[70]}')\nplt.imshow(augmented_images[70], cmap=\"gray\")\nplt.title(\"Figure 9: Augmented image sample\")\nplt.show","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:09:07.126988Z","iopub.execute_input":"2025-04-15T15:09:07.127327Z","iopub.status.idle":"2025-04-15T15:09:07.335949Z","shell.execute_reply.started":"2025-04-15T15:09:07.127300Z","shell.execute_reply":"2025-04-15T15:09:07.335045Z"},"papermill":{"duration":0.308498,"end_time":"2025-03-13T06:50:27.406980","exception":false,"start_time":"2025-03-13T06:50:27.098482","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.089173,"end_time":"2025-03-13T06:50:27.582597","exception":false,"start_time":"2025-03-13T06:50:27.493424","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### **Data modeling (The Verifier)**","metadata":{"papermill":{"duration":0.089833,"end_time":"2025-03-13T06:50:27.758552","exception":false,"start_time":"2025-03-13T06:50:27.668719","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"- Training Setup and configuration, callbacks: for better training setup three callbacks were used one that is used for early stopping based on the validation loss (minimization). One saves the model best weights called model checkpointing and finally the Reduce LR on Platea wich monitors the validation loss and changes to the goal of (minimzation) of it","metadata":{"papermill":{"duration":0.087914,"end_time":"2025-03-13T06:50:27.935258","exception":false,"start_time":"2025-03-13T06:50:27.847344","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Training Config:\nEPOCHS = 60\nBATCH_SIZE = 32\nloss = 'sparse_categorical_crossentropy'\n# optimizer = 'adam'\nactivation = \"leaky_relu\"\ninitial_learning_rate = 0.0010000000474974513\noptimizer = Adam(learning_rate=initial_learning_rate)\n\nearlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('.mdl_wts.keras', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:09:13.032543Z","iopub.execute_input":"2025-04-15T15:09:13.032866Z","iopub.status.idle":"2025-04-15T15:09:13.047249Z","shell.execute_reply.started":"2025-04-15T15:09:13.032838Z","shell.execute_reply":"2025-04-15T15:09:13.046295Z"},"papermill":{"duration":0.102843,"end_time":"2025-03-13T06:50:28.126733","exception":false,"start_time":"2025-03-13T06:50:28.023890","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Model Archeticticure:","metadata":{"papermill":{"duration":0.084697,"end_time":"2025-03-13T06:50:28.301907","exception":false,"start_time":"2025-03-13T06:50:28.217210","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_model():\n    \"\"\"\n    Create the model architicure and compile it, call on pre-set values.\n    Returns:\n        model (keras.Sequential): a model compiled with its layers\n    \"\"\"\n    padding = 'same'\n    poolpadding = 'valid'\n\n    model = Sequential([\n        Input(input_shape),\n        ####### Features extraction\n        \n        Conv2D(32, (5, 5), padding=padding, activation=activation, name=\"Conv1\"),\n        BatchNormalization(axis=-1, name=\"BN1\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool1\"),\n        GaussianNoise(0.1, name=\"GaussianNoise\"), \n        Dropout(0.1, name=\"Dropout1\"),\n\n        Conv2D(64, (5, 5), padding=padding, activation=activation, name=\"Conv2\"),\n        BatchNormalization(axis=-1, name=\"BN2\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool2\"),\n        Dropout(0.1, name=\"Dropout2\"),\n\n        Conv2D(128, (5, 5), padding=padding, activation=activation, name=\"Conv3\"),\n        BatchNormalization(axis=-1, name=\"BN3\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool3\"),\n        Dropout(0.25, name=\"Dropout3\"),\n\n        Conv2D(256, (3, 3), padding=padding, activation=activation, name=\"Conv4\"),\n        BatchNormalization(axis=-1, name=\"BN4\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool4\"),\n        Dropout(0.25, name=\"Dropout4\"),\n\n        Conv2D(256, (3, 3), padding=padding, activation=activation, name=\"Conv5\"),\n        BatchNormalization(axis=-1, name=\"BN5\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool5\"),\n        Dropout(0.25, name=\"Dropout5\"),\n        \n        Conv2D(512, (3, 3), padding=padding, activation=activation, name=\"Conv6\"),\n        BatchNormalization(axis=-1, name=\"BN6\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool6\"),\n        Dropout(0.45, name=\"Dropout6\"),\n        \n        Conv2D(512, (2, 2), padding=padding, activation=activation, name=\"Conv7\"),\n        BatchNormalization(axis=-1, name=\"BN7\"),  \n        MaxPooling2D(pool_size=(2, 2), padding=poolpadding, name=\"Mpool7\"),\n        Dropout(0.5, name=\"Dropout7\"),\n        \n        #### Flatten and fully connected layers, classifier using relu sofftmax\n        Flatten(),\n        Dense(128, activation=activation, name = \"Dense1\"),\n        Dense(2000, activation='softmax', name=\"SoftmaxClasses\"),\n    ],\n    name=\"IRISRecognizer\")\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:09:17.686967Z","iopub.execute_input":"2025-04-15T15:09:17.687302Z","iopub.status.idle":"2025-04-15T15:09:17.695918Z","shell.execute_reply.started":"2025-04-15T15:09:17.687274Z","shell.execute_reply":"2025-04-15T15:09:17.694953Z"},"papermill":{"duration":0.096347,"end_time":"2025-03-13T06:50:28.483189","exception":false,"start_time":"2025-03-13T06:50:28.386842","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Model Training and testing:","metadata":{"papermill":{"duration":0.085265,"end_time":"2025-03-13T06:50:28.653607","exception":false,"start_time":"2025-03-13T06:50:28.568342","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = create_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:09:23.821466Z","iopub.execute_input":"2025-04-15T15:09:23.821779Z","iopub.status.idle":"2025-04-15T15:09:25.311742Z","shell.execute_reply.started":"2025-04-15T15:09:23.821754Z","shell.execute_reply":"2025-04-15T15:09:25.311097Z"},"papermill":{"duration":1.7188,"end_time":"2025-03-13T06:50:30.461591","exception":false,"start_time":"2025-03-13T06:50:28.742791","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(np.array(x_train), y_train, validation_data=(np.array(x_valid), y_valid), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[earlyStopping, mcp_save, reduce_lr_loss])","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:09:31.356431Z","iopub.execute_input":"2025-04-15T15:09:31.356837Z","iopub.status.idle":"2025-04-15T15:33:07.278486Z","shell.execute_reply.started":"2025-04-15T15:09:31.356804Z","shell.execute_reply":"2025-04-15T15:33:07.277676Z"},"papermill":{"duration":976.650209,"end_time":"2025-03-13T07:06:47.200811","exception":false,"start_time":"2025-03-13T06:50:30.550602","status":"completed"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model has used the Early stopping checkpoin at the 66th epoch it was no longer updating on the accuracy.","metadata":{"papermill":{"duration":0.663502,"end_time":"2025-03-13T07:06:48.449056","exception":false,"start_time":"2025-03-13T07:06:47.785554","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Model Performance (Loss and Accuracy)","metadata":{"papermill":{"duration":0.604511,"end_time":"2025-03-13T07:06:49.657682","exception":false,"start_time":"2025-03-13T07:06:49.053171","status":"completed"},"tags":[]}},{"cell_type":"code","source":"plt.figure()\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\n\nplt.title('Figure 10: Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:33:20.814067Z","iopub.execute_input":"2025-04-15T15:33:20.814373Z","iopub.status.idle":"2025-04-15T15:33:20.991892Z","shell.execute_reply.started":"2025-04-15T15:33:20.814337Z","shell.execute_reply":"2025-04-15T15:33:20.991173Z"},"papermill":{"duration":0.796652,"end_time":"2025-03-13T07:06:51.088817","exception":false,"start_time":"2025-03-13T07:06:50.292165","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model is training since with the next epochs the accuracy is getting higher and higher until it reacahes a stable accuracu between 80 and 90%. It is noticed that the are some fluctuating in the accuarcy between the 10th epoch until 33rd epoch. But it is interpreted as it doesn't affect the performace, It could be due the use of dropouts techniques with the use of batch normalization technique.","metadata":{"papermill":{"duration":0.588598,"end_time":"2025-03-13T07:06:52.314825","exception":false,"start_time":"2025-03-13T07:06:51.726227","status":"completed"},"tags":[]}},{"cell_type":"code","source":"training_loss = history.history['loss']\nvalidation_loss = history.history['val_loss']\nepochs = range(1, len(training_loss) + 1)\n\nplt.plot(epochs, training_loss, label='Training Loss')\nplt.plot(epochs, validation_loss, label='Validation Loss')\nplt.title('Figure 11: Loss Learning Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:33:26.499457Z","iopub.execute_input":"2025-04-15T15:33:26.499745Z","iopub.status.idle":"2025-04-15T15:33:26.675511Z","shell.execute_reply.started":"2025-04-15T15:33:26.499722Z","shell.execute_reply":"2025-04-15T15:33:26.674769Z"},"papermill":{"duration":0.959024,"end_time":"2025-03-13T07:06:53.915397","exception":false,"start_time":"2025-03-13T07:06:52.956373","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The loss learning curve shows that along the epochs the loss gets lower and lower until it's under 0.1, however it's noticed that in the first 30 epochs there are fluctuating in the validation loss, it's not smooth such as the training one. ","metadata":{"papermill":{"duration":0.640925,"end_time":"2025-03-13T07:06:55.174866","exception":false,"start_time":"2025-03-13T07:06:54.533941","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Testing and save model weights","metadata":{"papermill":{"duration":0.6482,"end_time":"2025-03-13T07:06:56.429805","exception":false,"start_time":"2025-03-13T07:06:55.781605","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.save(\"IRISRecognizer.h5\")\nreconstructed_model = keras.models.load_model(\"IRISRecognizer.h5\")\ntest_loss, test_acc = reconstructed_model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_acc*100:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:33:30.970550Z","iopub.execute_input":"2025-04-15T15:33:30.970834Z","iopub.status.idle":"2025-04-15T15:33:34.567187Z","shell.execute_reply.started":"2025-04-15T15:33:30.970812Z","shell.execute_reply":"2025-04-15T15:33:34.566267Z"},"papermill":{"duration":4.363258,"end_time":"2025-03-13T07:07:01.387826","exception":false,"start_time":"2025-03-13T07:06:57.024568","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Predictions on sample images","metadata":{"papermill":{"duration":0.599795,"end_time":"2025-03-13T07:07:02.639696","exception":false,"start_time":"2025-03-13T07:07:02.039901","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def evaluate_model_visualize(dataset, y_true, model, class_names):\n    \"\"\"\n    Make a sample and predict it using the model visualize outputs and calculate the accuracy.\n    Args:\n        - dataaset (np.array): x_test dataset\n        - y_true (np.array): y_test dataset\n        - model (keras.Sequential): weighted trained model\n        - class_names (pd.Series): classes in the dataset\n    Return:\n        - Nothing.\n    \"\"\"\n    num_rows = 4\n    num_cols = 6\n    data_batch = dataset[0:num_rows * num_cols]\n    predictions = model.predict(data_batch)\n    plt.figure(figsize=(20, 8))\n    num_matches = 0\n\n    for idx in range(num_rows * num_cols):\n        ax = plt.subplot(num_rows, num_cols, idx + 1)\n        plt.axis(\"off\")\n        plt.imshow(data_batch[idx], cmap=\"gray\")\n\n        pred_idx = np.argmax(predictions[idx])\n        true_idx = y_true[idx]\n\n        title = f\"{class_names[true_idx]} : {class_names[pred_idx]}\"\n        title_obj = plt.title(title, fontdict={\"fontsize\": 13})\n\n        if pred_idx == true_idx:\n            num_matches += 1\n            plt.setp(title_obj, color=\"g\")\n        else:\n            plt.setp(title_obj, color=\"r\")\n    acc = num_matches / (num_rows * num_cols)\n    print(\"Prediction accuracy: {:.2%}\".format(acc))","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:33:37.733391Z","iopub.execute_input":"2025-04-15T15:33:37.733681Z","iopub.status.idle":"2025-04-15T15:33:37.739655Z","shell.execute_reply.started":"2025-04-15T15:33:37.733659Z","shell.execute_reply":"2025-04-15T15:33:37.738774Z"},"papermill":{"duration":0.627325,"end_time":"2025-03-13T07:07:03.918298","exception":false,"start_time":"2025-03-13T07:07:03.290973","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model_visualize(x_test,y_test, reconstructed_model,df['Label'].unique())","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:33:43.627542Z","iopub.execute_input":"2025-04-15T15:33:43.627823Z","iopub.status.idle":"2025-04-15T15:33:46.838971Z","shell.execute_reply.started":"2025-04-15T15:33:43.627801Z","shell.execute_reply":"2025-04-15T15:33:46.838057Z"},"papermill":{"duration":3.678541,"end_time":"2025-03-13T07:07:08.261273","exception":false,"start_time":"2025-03-13T07:07:04.582732","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"From this prediction sample the accuracy of the model on classifiying the sample has reached 87.5%. We have two red sample that were wrongly classified, however it is noticed that the model is able to actually tell if the eye image is a left eye or a right eye wich is a good thing so far.","metadata":{"papermill":{"duration":0.659617,"end_time":"2025-03-13T07:07:09.520540","exception":false,"start_time":"2025-03-13T07:07:08.860923","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**Threshold decision:** the decision is based on the confidence level of the model on their classifications, the maximum value minus the minimum value will give a good threshold.","metadata":{"papermill":{"duration":0.67018,"end_time":"2025-03-13T07:07:10.802704","exception":false,"start_time":"2025-03-13T07:07:10.132524","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def describe_model_confedince(reconstructed_model, x_test): \n    \"\"\"\n    This is to look into the distribution of the maximum liklihoods\n    represnts the confidence levels of the how much the model is sure this is (the classification)\n    Args:\n        - reconstructed_model(keras.Sequential): trained weights\n        - x_test(np.array): testing images\n    Returns:\n        Nothing, only prints statics about the max_liklihoods\n    \"\"\"\n    probabilites = reconstructed_model.predict(x_test)\n    y_pred = np.argmax(probabilites, axis=1)\n    max_liklihoods = np.max(probabilites, axis=1)\n    print(\"Average of (Maximum Liklihood) for predictions: \", np.average(max_liklihoods))\n    print(\"Standard deviation of (Maximum Liklihood) for predictions: \", np.std(max_liklihoods))\n    print(\"Median of (Maximum Liklihood) for predictions: \", np.median(max_liklihoods))\n    print(\"Minimum of (Maximum Liklihood) for predictions: \", np.min(max_liklihoods))\n    print(\"Maximum of (Maximum Liklihood) for predictions: \", np.max(max_liklihoods))\n    threshold = np.max(max_liklihoods) - np.min(max_liklihoods)\n    print(\"Chosen threshold is: \", threshold)\n    return probabilites","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:12.042730Z","iopub.status.busy":"2025-03-13T07:07:12.042416Z","iopub.status.idle":"2025-03-13T07:07:12.048090Z","shell.execute_reply":"2025-03-13T07:07:12.047340Z"},"jupyter":{"source_hidden":true},"papermill":{"duration":0.617896,"end_time":"2025-03-13T07:07:12.049399","exception":false,"start_time":"2025-03-13T07:07:11.431503","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilites = describe_model_confedince(reconstructed_model, x_test)","metadata":{"execution":{"iopub.status.busy":"2025-04-15T15:34:10.153157Z","iopub.execute_input":"2025-04-15T15:34:10.153484Z","iopub.status.idle":"2025-04-15T15:34:10.175004Z","shell.execute_reply.started":"2025-04-15T15:34:10.153456Z","shell.execute_reply":"2025-04-15T15:34:10.173825Z"},"papermill":{"duration":2.90588,"end_time":"2025-03-13T07:07:15.607426","exception":false,"start_time":"2025-03-13T07:07:12.701546","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- The avergae maximum probabiliy is (0.9541976). The average is a high one that means that the model is more certain or confidence about the higher probaility of each class.\n- The median is also very high value (0.9999617). \n- The maximum is (1)","metadata":{"papermill":{"duration":0.595628,"end_time":"2025-03-13T07:07:16.914997","exception":false,"start_time":"2025-03-13T07:07:16.319369","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Model Evaluaion metrics:\nThe evaluation is done using the required metrics:\n- **False Match Rate (FMR):** The rate at which an imposter is incorrectly classified as a genuine user. \n- **False Non-Match Rate (FNMR):** The rate at which a genuine user is incorrectly classified as an imposter. \n- **Plot the Receiver Operating Characteristic (ROC) curve**, which shows the trade-off between FMR and FNMR. \n- **Determine the Equal Error Rate (EER)**, the point on the ROC curve where FMR and FNMR are equal. ","metadata":{"papermill":{"duration":0.600652,"end_time":"2025-03-13T07:07:18.168246","exception":false,"start_time":"2025-03-13T07:07:17.567594","status":"completed"},"tags":[]}},{"cell_type":"code","source":"precision = Precision(task=\"multiclass\", num_classes=2000)\nrecall = Recall(task=\"multiclass\", num_classes=2000)\naccuracy = Accuracy(task=\"multiclass\", num_classes=2000)\nmaurc = AUROC(task=\"multiclass\",num_classes=2000)","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:19.427828Z","iopub.status.busy":"2025-03-13T07:07:19.427500Z","iopub.status.idle":"2025-03-13T07:07:19.481353Z","shell.execute_reply":"2025-03-13T07:07:19.480612Z"},"papermill":{"duration":0.665264,"end_time":"2025-03-13T07:07:19.482895","exception":false,"start_time":"2025-03-13T07:07:18.817631","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracyv = accuracy(torch.tensor(probabilites), torch.tensor(y_test))\nprec = precision(torch.tensor(probabilites), torch.tensor(y_test))\nrec = recall(torch.tensor(probabilites), torch.tensor(y_test))\naurc = maurc(torch.tensor(probabilites), torch.tensor(y_test))","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:20.738158Z","iopub.status.busy":"2025-03-13T07:07:20.737808Z","iopub.status.idle":"2025-03-13T07:07:22.041258Z","shell.execute_reply":"2025-03-13T07:07:22.040480Z"},"papermill":{"duration":1.950936,"end_time":"2025-03-13T07:07:22.042991","exception":false,"start_time":"2025-03-13T07:07:20.092055","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Accuracy: {accuracyv}\")\nprint(f\"Precision: {prec}\")\nprint(f\"Recall: {rec}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:23.309453Z","iopub.status.busy":"2025-03-13T07:07:23.309100Z","iopub.status.idle":"2025-03-13T07:07:23.313990Z","shell.execute_reply":"2025-03-13T07:07:23.313021Z"},"papermill":{"duration":0.672231,"end_time":"2025-03-13T07:07:23.315420","exception":false,"start_time":"2025-03-13T07:07:22.643189","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nnum_classes = 2000\n\nfor i in range(num_classes):\n    y_true_binary = np.where(y_test == i, 1, 0)\n    fpr[i], tpr[i], _ = roc_curve(y_true_binary, probabilites[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-03-13T07:07:24.599923Z","iopub.status.busy":"2025-03-13T07:07:24.599580Z","iopub.status.idle":"2025-03-13T07:07:26.282422Z","shell.execute_reply":"2025-03-13T07:07:26.281323Z"},"papermill":{"duration":2.34035,"end_time":"2025-03-13T07:07:26.284218","exception":false,"start_time":"2025-03-13T07:07:23.943868","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  ## this is the state of a random classifier\n\nfor i in range(num_classes):\n    if not math.isnan(roc_auc[i]):\n        plt.plot(fpr[i], tpr[i], lw=2)#, label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n        \nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Figure: Receiver Operating Characteristic (ROC) Curve for Each Class')\nplt.legend(loc=\"lower right\")\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:27.485413Z","iopub.status.busy":"2025-03-13T07:07:27.485089Z","iopub.status.idle":"2025-03-13T07:07:29.119933Z","shell.execute_reply":"2025-03-13T07:07:29.119107Z"},"papermill":{"duration":2.23306,"end_time":"2025-03-13T07:07:29.121616","exception":false,"start_time":"2025-03-13T07:07:26.888556","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"eer_thresholds = []\n\nfor i in range(num_classes):\n    try:\n        fpr_values = fpr[i]\n        tpr_values = tpr[i]\n        interp_fn = interp1d(fpr_values, tpr_values)\n        def eer_fn(x):\n            return 1.0 - x - interp_fn(x)\n\n        eer_threshold = brentq(eer_fn, 0.0, 1.0)\n        eer_thresholds.append(eer_threshold)\n    except ValueError:\n        continue\n\navg_eer_threshold = np.mean(eer_thresholds)\nprint(f\"Equal Error Rate (EER) Threshold: {avg_eer_threshold:.4f}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:30.375802Z","iopub.status.busy":"2025-03-13T07:07:30.375478Z","iopub.status.idle":"2025-03-13T07:07:30.583417Z","shell.execute_reply":"2025-03-13T07:07:30.582500Z"},"papermill":{"duration":0.820492,"end_time":"2025-03-13T07:07:30.584925","exception":false,"start_time":"2025-03-13T07:07:29.764433","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Model Exporting \nThe environemnt which the model will be used to do infernece on is a mobile application environmnet. The form that the model is chosen to be exported into is tflite.","metadata":{"papermill":{"duration":0.667599,"end_time":"2025-03-13T07:07:31.844398","exception":false,"start_time":"2025-03-13T07:07:31.176799","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# tf.saved_model.save(reconstructed_model, './model')\n# coverter = tf.lite.TFLiteConverter.from_saved_model('./model')\n# tflite_model = coverter.convert()\n# tflite_model_file = pathlib.Path('./model/model.tflite')\n# tflite_model_file.write_bytes(tflite_model)","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:33.109058Z","iopub.status.busy":"2025-03-13T07:07:33.108707Z","iopub.status.idle":"2025-03-13T07:07:33.112214Z","shell.execute_reply":"2025-03-13T07:07:33.111319Z"},"papermill":{"duration":0.662881,"end_time":"2025-03-13T07:07:33.113639","exception":false,"start_time":"2025-03-13T07:07:32.450758","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# interpreter = tf.lite.Interpreter(model_content=tflite_model)\n# interpreter.allocate_tensors()\n# input_details = interpreter.get_input_details()\n# output_details = interpreter.get_output_details()\n# input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n\n# interpreter.set_tensor(input_details[0]['index'], input_data)\n# interpreter.invoke()\n# tflite_results = interpreter.get_tensor(output_details[0]['index'])","metadata":{"execution":{"iopub.execute_input":"2025-03-13T07:07:34.435198Z","iopub.status.busy":"2025-03-13T07:07:34.434850Z","iopub.status.idle":"2025-03-13T07:07:34.438667Z","shell.execute_reply":"2025-03-13T07:07:34.437908Z"},"papermill":{"duration":0.651181,"end_time":"2025-03-13T07:07:34.440016","exception":false,"start_time":"2025-03-13T07:07:33.788835","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.663214,"end_time":"2025-03-13T07:07:35.730559","exception":false,"start_time":"2025-03-13T07:07:35.067345","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### **User Experinece and GUI (The Authenticater)**\nFor the enrollemnt module, a mobile application is chosen for it's conveniency to take picture of the IRIS or upload the image itslef. The model is exported to be in a mobile application environmnet. ","metadata":{"papermill":{"duration":0.656674,"end_time":"2025-03-13T07:07:36.995683","exception":false,"start_time":"2025-03-13T07:07:36.339009","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### First time users, and Prefrences\nThis is the activities that shows when the user opens the app wether they are first time users or not. They will either enroll or authenticate if the phone is not a first time user.\n\n<div align=\"center\">\n<img src=\"https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/024181f1-7a0f-40f7-ba5d-b63ba90757a4\" width=\"220\"><img src=\"https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/b45dce95-257a-42e7-a885-7fdf85de2405\" width=\"220\">\n</div>\n\nFigure 12: Prefrences","metadata":{"papermill":{"duration":0.594463,"end_time":"2025-03-13T07:07:38.185660","exception":false,"start_time":"2025-03-13T07:07:37.591197","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Image Accuisition\nThe system does image aquisition in two forms, once for enrollemnt and the other is for authintication. As shown bellow there is two options, you either authnticate or enroll. To authnticate you can only use the Camera, to enroll the user can only use the gallery since there will have to chose images for the left eyes and images for the right eye.\n\n<div align=\"center\">\n    <img src=\"https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/f2b46d84-4eb5-44e8-9caa-a1cdb63dcf30\" width=\"220\">\n    <img src=\"https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/55fa7079-5f32-461a-b8b4-707dc0244b5d\" width=\"220\">\n</div>\n\nFigure 13: Image acusition","metadata":{"papermill":{"duration":0.597308,"end_time":"2025-03-13T07:07:39.435520","exception":false,"start_time":"2025-03-13T07:07:38.838212","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Image Preparing for Recognition\n\nThe image must be prepared in the same way as the image was prepared for training. So once the users input their Iris image it will go through image processing. Such as resizing and normalization.","metadata":{"papermill":{"duration":0.644344,"end_time":"2025-03-13T07:07:40.682773","exception":false,"start_time":"2025-03-13T07:07:40.038429","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Model Inference\nFinally, after the image is aquired for recognition, the model infernce is done to it. The output will be the probalities or (confidences rates). The highest probablbe value will be then compared to the threshold that we obtained from the model confedince level, if teh highest one is less than the threshold then it will be not accepted and the user is redirected to anotehr screen, othersiwe they will be accepted with the label of them shown and the condince level is shown. \n\nFor the enrollment, model inference will not be done, instead the model will be doing countinous training and the enrollment images are then added to the dataset and the model will retrain on them. So that it will be able to Identify the new (label).\n\nOnce the infernce answer is there, the user will see one of these outputs depending if the model infernce passed the threshold:\n\n<div align=\"center\">\n    <img src=\"https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/7c7bf3f0-d8e0-4369-b207-a094c3c1f690\" width=\"220\">\n    <img src=\"https://github.com/sondosaabed/Deep-Learning-for-Text/assets/65151701/1a360593-1ced-48a0-b5bb-24d59aa3fc1b\" width=\"220\">\n</div>\n\nFigure 14: Model Answer","metadata":{"papermill":{"duration":0.638135,"end_time":"2025-03-13T07:07:41.935509","exception":false,"start_time":"2025-03-13T07:07:41.297374","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Discussion and Optimization**\n\nAs in any software product, there is alwyas a room for optimization. Despite obtaining great accuracy results (90% on testing dataset), and having an average of confidence levels high as 95.41976% \nsuch authentication system are critical and in my opinion it must reach the state of the art such as 99% of accuracy in testing and generalizing in new dataset. \n\nSince this is a data driven project, of course having a diffrent dataset of people. Such as middle eastern ethnicties, african americans. The model could have bias, this would lead to bias authentications if new enrolled people have dark skin. Samples were defintly white asian people. This is because it's an end-to-end approach it doesn't inlude segmentation so the area around the Iris (the whole eye and a little face is shown).","metadata":{"papermill":{"duration":0.648664,"end_time":"2025-03-13T07:07:43.189124","exception":false,"start_time":"2025-03-13T07:07:42.540460","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.648215,"end_time":"2025-03-13T07:07:44.475420","exception":false,"start_time":"2025-03-13T07:07:43.827205","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Conclusion**\nIn conclusion, a biometric authentication system was designed in this project. First of all, the dataset was chosen and goes through data analysis process were it was looked into in terms of distribution of images and labels, duplicates and missing of corrupted images. Then it was prepared using bare minimum image processing such as resizing with keeping the aspect ratio and normalization. The labels were transformed into codes. The model architicture is designed using Deep Convultional neural network, fully connected layers and softmaxs. Finally the model was trained, evaluated and exported. A simple mobile application was designed and the model was deployed on it to do inference of the model. Meanwhile the enrollment part is left as an optimization to the system, teh idea is to use countinuous training.\n<hr>","metadata":{"papermill":{"duration":0.648609,"end_time":"2025-03-13T07:07:45.735355","exception":false,"start_time":"2025-03-13T07:07:45.086746","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 🔴 **Cite This Notebook** 🔴\n\nIf you find this notebook useful in your research or projects, please consider citing it. Proper citation helps me gain recognition for my work and allows others to follow and build upon it.\n\n**Sondos, _An End-to-end segmentation-free approach Iris Biometric Authentication_, Open Source (GitHub & Kaggle), May 2024. Available at: [https://github.com/sondosaabed/Iris-of-eyes-recognition](https://github.com/sondosaabed/Iris-of-eyes-recognition) and [https://www.kaggle.com/code/sondosaabed/iris-eye-recognition-endtoend-93](https://www.kaggle.com/code/sondosaabed/iris-eye-recognition-endtoend-93)**\n<hr>\n","metadata":{"papermill":{"duration":0.625261,"end_time":"2025-03-13T07:07:46.961982","exception":false,"start_time":"2025-03-13T07:07:46.336721","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## **Refrences**\n- [1] [Deep Learning for Iris Recognition: A Review, Yin, Y., He, S., Zhang, R., Chang, H., Han, X., & Zhang, J. (2024)](https://arxiv.org/abs/2303.08514)\n- [2] https://machinelearningmastery.com/how-to-prepare-categorical-data-for-deep-learning-in-python/\n- [3] https://hycasia.github.io/dataset/casia-irisv4/\n- [4] https://statisticallyrelevant.com/confusion-matrix-and-roc-curves/\n- [5] https://www.researchgate.net/publication/327288671_The_Impact_of_Preprocessing_on_Deep_Representations_for_Iris_Recognition_on_Unconstrained_Environments\n- [6] https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal\n- [7] https://www.v7labs.com/blog/confusion-matrix-guide\n- [8] https://builtin.com/machine-learning/siamese-network\n- [9] https://www.kaggle.com/code/nkitgupta/evaluation-metrics-for-multi-class-classification\n- [10] https://lightning.ai/docs/torchmetrics/stable/classification/auroc.html\n- [11] https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n- [12] https://github.com/sondosaabed/Introduction-to-Tensorflow-lite/blob/main/notebooks/Intro%20Code%20Examples.ipynb","metadata":{"papermill":{"duration":0.616333,"end_time":"2025-03-13T07:07:48.243154","exception":false,"start_time":"2025-03-13T07:07:47.626821","status":"completed"},"tags":[]}}]}